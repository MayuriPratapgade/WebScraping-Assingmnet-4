{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e4c60c-924e-4d1f-b69c-ee805dac48d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "                 WebScraping Assignmnet  4\n",
    "\n",
    "   Q. 2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "improt request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url= \"https://www.bcci.tv/\"\n",
    "soup=\n",
    "BeutifulSoup(response.content,\n",
    " \"html.parser\")\n",
    "fixtur/e_link=soup.find(\"a\",text=\"International Fixtures\")\n",
    "[\"href\"]\n",
    "\n",
    "fixture_url=url+\n",
    "fixtures_link\n",
    "fixtures_response=\n",
    "requests.get(fixture_url)\n",
    "fixtures_soup=\n",
    "BaeutifulSoup(fixtures_response.\n",
    " \"html.parser\")\n",
    " fixtures=\n",
    "fixtures_soup.find_all(\"div\",\n",
    " class_=\"fixture_formet-\n",
    "for fixture in fixtures:\n",
    "series =\n",
    "fixture.find(\"span\",\n",
    " class_=\"u-unskewed-\n",
    " text\").text.strip()\n",
    " palce = fixture.find(\"p\"\n",
    " class_=\"fixture__additional-\n",
    " info\").text.strip()\n",
    " date=fixture.find(\"span\",\n",
    " classa-a'fixture__date\").text.strip\n",
    " time=fixture.find(\"span\",\n",
    " class_=\"fixture__time\").text.strip\n",
    "\n",
    " print(\"Series:\", series)\n",
    " print(\"Plase:\", plase)\n",
    " print(\"Date:\", date)\n",
    " print(\"Time:\", time)\n",
    " print()\n",
    " \n",
    "\n",
    " \n",
    "                       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fcbc9f-e77e-445c-9ddb-6ab8c9d7ceb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q  4\n",
    "\n",
    "import request\n",
    "from bs4 import BeautifulSoup\n",
    "import pprint\n",
    "\n",
    "\n",
    "\n",
    "def get_trending_repositores():\n",
    "    url_to_call = \"https://github.com/trending\"\n",
    "    response = request.get(url_to_cal,headers=[\"User-Agent\":\"Mozilla/5.0\"})\n",
    "    response_code =200:\n",
    "    print(\"Error occurred\")\n",
    "    return\n",
    "    html_content = response.content\n",
    "    dom = BeautifulSoup(html_content,\"html.parser\")\n",
    "    all_trainding_repos = dom.select(\"article.Box-row h1\")\n",
    "    trainding_repositories = []\n",
    "    for each_trainding_repo in all_trainding_repos:\n",
    "       hreh_link = each_trainding_repo.a.attrs[\"href\"]\n",
    "        name = href_link[1:]\n",
    "        repo = {\"labe\":name,\n",
    "                \"link\":http://github.com{}\".format(href_link)}\n",
    "        trainding_repositories.append(repo)\n",
    "     return tranding_repositories\n",
    "        \n",
    "\n",
    "\n",
    "   if__name__=\"_main__\":\n",
    "    print(\"Started scraping\")\n",
    "  tranding_repos = get_tranding_repositories()\n",
    "  pprint.pprint(tranding_repos)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a9ce8a-3e1a-4024-a0af-57811eab38d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q  5\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d0470a-442c-45dc-a451-cb2f575d6eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q  6\n",
    "\n",
    "\n",
    "\n",
    "import request \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send a GET request to the \n",
    "   URL \n",
    "url=\n",
    "\"https://www.theguardian.com/news/datalog/2012/aug/09/best-\n",
    "selling-books-all-time-fifty-\n",
    "shades-grey-compare\"\n",
    "respose = requests.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup =\n",
    "BeautifulSoup(response.content,\n",
    " 'html.parser')\n",
    "# Find the relevent HTML\n",
    "elements and extract the data\n",
    " novel = []\n",
    "table = soup. find('table')\n",
    "rows = table . find_all('tr')\n",
    "[1:] # Exclude the header row\n",
    "\n",
    "\n",
    "for row in row :\n",
    " columns = row.find_all('td')\n",
    "    book_name =\n",
    "  columns[1]  \n",
    "                           \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8becea19-81eb-4fc0-b766-06f3f2374109",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q   7\n",
    "\n",
    "\n",
    "\n",
    "import request \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url =\n",
    "\"https://www.imdb.com/list/Is095964455\n",
    "soup =\n",
    "BeautifulSoup(response.content,\n",
    "    \"html.parser\")\n",
    "series_list =\n",
    "soup.find_all(\"div\",\n",
    " class_=\"lister-item-content\")\n",
    " for series in series_list:\n",
    "     name =\n",
    "     series.find(\"h3\").find(\"a\").text.\n",
    "     year_span =\n",
    "     series.find(\"span\",\n",
    "     class_=\"lister-item-\n",
    "     year\").text.strip(\"()\")\n",
    "\n",
    "   genre = series. find(\"span\",\n",
    "   class_=\"genre\").tex.strip()\n",
    "   runtime=\n",
    "   series.find(\"span\",\n",
    "   class_==\"rintime\").text.strip()\n",
    "   rating = series.find(\"span\",\n",
    " calss_=\"ipl-rating-\n",
    " star__rating\").text.strip()\n",
    " votes = series.find(\"span\",\n",
    " attrs={\"name\":\n",
    " \"nv\"}).text.strip()\n",
    "\n",
    " print(\"Name:\", name)\n",
    " print(\"Year Span:\", year span)\n",
    "\n",
    "   print(\"Genre:\", genre)\n",
    "   print(\"Run Time:\", runtime)\n",
    "   print(\"Rating:\", rating)\n",
    "   print(\"Votes:\", votes)\n",
    "   print()\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e67c2e-2b5e-4dad-aec3-c515aea36450",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q 8\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import beautiulfulSoup\n",
    "#  Send a GET  request to the \n",
    "UCI machine learning\n",
    "repositories website\n",
    "url=\n",
    "\"https://archive.ics.uci.edu/\"\n",
    "response = request.get(url)\n",
    "\n",
    "# parse the HTML content using \n",
    "  BeautifulSoup\n",
    " soup = \n",
    "BeautifulSoup(response.content,\n",
    "  \"html.parser\")\n",
    " # find the link to the \"show\n",
    "  All Dataset\" page\n",
    "show_all_link = soup.find(\"a\",\n",
    "  href=\"ml/datasetss.php\")\n",
    "#  Contruct the URL for the \n",
    "  \"Show All Dataset\" page\n",
    "   show_all_url = url +\n",
    "   show_all_link[\"herf\"]\n",
    "\n",
    "# Send another GET request to the \n",
    "    \"Show All Dataset\" page\n",
    "  show_all_response =\n",
    "requests.get(show_all_url)\n",
    "\n",
    "# Parse the HTML content of the \n",
    "\"Show All Dataset\" page\n",
    "show_all_soup =\n",
    "BeautifulSoup(show_all_response)\n",
    "show_all_soup.find(\"table\",\n",
    " class_\"table\")\n",
    "\n",
    "# Extract the details from the table rows\n",
    "dataset_details = []\n",
    "for row in \n",
    "dataset_table.find_all(\"tr\")\n",
    "[1:]\n",
    "columns = row.find_all(td\")\n",
    " dataset_name = \n",
    "  columns[0].text.strip()\n",
    "     data_type = \n",
    "columns[1].text.strip()\n",
    "    task =\n",
    "columns[2].text.strip()\n",
    "      attribute_type =\n",
    "columns[3].text.strip()\n",
    "      num_instances =\n",
    "columns[4].text.strip()\n",
    "   num_attributes =\n",
    "columns[5].text.strip()\n",
    "      yesr =\n",
    " columns[6].text.strip()\n",
    "dataset_details,append((dataset_)\n",
    "    data_type, task,)  \n",
    "# Print the dataset details\n",
    "   for dataset in \n",
    "     dataset_details:\n",
    "   print(\"Dataset Name:\",\n",
    "         dataset[0])\n",
    "    print(\"Data Type:\",\n",
    "          dataset[1])\n",
    "    print(\"Task:\",dataset[2])\n",
    "    print(\"Attribute type :\",\n",
    "  dataset[3])\n",
    "    print(\"No of Instance:\",\n",
    "   dataset[4])\n",
    "     print(\" No of Attribute :\",\n",
    "     dataset[5])\n",
    "     print(\"yesr:\", dataset[6])\n",
    "          print()\n",
    "          \n",
    "            \n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "          \n",
    "          \n",
    "\n",
    "              \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
